{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc769c5",
   "metadata": {},
   "source": [
    "# CapsWriter-Offline 独立热词与纠错系统 (Portable Standalone)\n",
    "\n",
    "本 Notebook 完整整合了以下核心逻辑，**逻辑分支与原始代码库完全对等**：\n",
    "- **音素处理** (algo_phoneme)\n",
    "- **相似度算法** (algo_calc)\n",
    "- **FastRAG 加速检索** (rag_fast)\n",
    "- **拼音纠错** (PhonemeCorrector, 包含 `similar_threshold` 相关逻辑)\n",
    "- **纠错历史 RAG** (RectificationRAG)\n",
    "- **调试工具** (Phoneme Debug)\n",
    "- **LLM 集成** (Prompt Builder & Ollama Client)\n",
    "\n",
    "**依赖安装：**\n",
    "\n",
    "```bash\n",
    "pip install pypinyin numba numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55132cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import threading\n",
    "import logging\n",
    "from typing import List, Tuple, Dict, Set, Union, Literal, Optional, NamedTuple\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 确保控制台输出 UTF-8\n",
    "if sys.platform == 'win32':\n",
    "    if hasattr(sys.stdout, 'reconfigure'):\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "\n",
    "# --- 依赖库导入 ---\n",
    "try:\n",
    "    from pypinyin import pinyin, Style\n",
    "except ImportError:\n",
    "    pinyin = None\n",
    "    Style = None\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    HAS_NUMPY = True\n",
    "except ImportError:\n",
    "    HAS_NUMPY = False\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "    HAS_NUMBA = True\n",
    "except ImportError:\n",
    "    HAS_NUMBA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170e4d3",
   "metadata": {},
   "source": [
    "## 1. 核心模型与音素处理 (algo_phoneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37550be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True, slots=True)\n",
    "class Phoneme:\n",
    "    value: str\n",
    "    lang: Literal['zh', 'en', 'num', 'other']\n",
    "    is_word_start: bool = False\n",
    "    is_word_end: bool = False\n",
    "    char_start: int = 0\n",
    "    char_end: int = 0\n",
    "\n",
    "    @property\n",
    "    def is_tone(self) -> bool: return self.value.isdigit()\n",
    "    \n",
    "    @property\n",
    "    def info(self) -> Tuple[str, str, bool, bool, bool, int, int]:\n",
    "        return (self.value, self.lang, self.is_word_start, self.is_word_end, self.is_tone, self.char_start, self.char_end)\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    result = []; prev_char = ''\n",
    "    for char in text:\n",
    "        if char.isalnum() or '\\u4e00' <= char <= '\\u9fff':\n",
    "            if char.isupper() and prev_char.islower(): result.append(' ')\n",
    "            elif char.isdigit() and prev_char.isalpha(): result.append(' ')\n",
    "            elif char.isalpha() and prev_char.isdigit(): result.append(' ')\n",
    "            result.append(char.lower()); prev_char = char\n",
    "        else:\n",
    "            if result and result[-1] != ' ': result.append(' ')\n",
    "            prev_char = ''\n",
    "    return ''.join(result).strip()\n",
    "\n",
    "\n",
    "def split_mixed_label(input_str: str) -> List[str]:\n",
    "    tokens = []; s = input_str.lower()\n",
    "    while len(s) > 0:\n",
    "        if s[0] == ' ': s = s[1:]; continue\n",
    "        match = re.match(r'[a-z]+', s)\n",
    "        if match: tokens.append(match.group(0)); s = s[len(match.group(0)):]\n",
    "        else:\n",
    "            match = re.match(r'[0-9]+', s)\n",
    "            if match: tokens.append(match.group(0)); s = s[len(match.group(0)):]\n",
    "            else: tokens.append(s[0]); s = s[1:]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_phoneme_seq(text: str) -> List[Phoneme]:\n",
    "    normalized = normalize_text(text)\n",
    "    seq = []\n",
    "    for token in split_mixed_label(normalized):\n",
    "        if re.match(r'^[a-z0-9]+$', token):\n",
    "            lang = 'num' if token.isdigit() else 'en'\n",
    "            seq.append(Phoneme(token, lang, is_word_start=True, is_word_end=True))\n",
    "        elif len(token) == 1:\n",
    "            if not pinyin: seq.append(Phoneme(token, 'zh', is_word_start=True, is_word_end=True))\n",
    "            else:\n",
    "                try:\n",
    "                    pi = pinyin(token, style=Style.INITIALS, strict=False)\n",
    "                    pf = pinyin(token, style=Style.FINALS, strict=False)\n",
    "                    pt = pinyin(token, style=Style.TONE3, neutral_tone_with_five=True)\n",
    "                    has_init = pi and pi[0] and pi[0][0]\n",
    "                    if has_init: seq.append(Phoneme(pi[0][0], 'zh', is_word_start=True))\n",
    "                    if pf and pf[0] and pf[0][0]: seq.append(Phoneme(pf[0][0], 'zh', is_word_start=not has_init))\n",
    "                    tone = pt[0][0][-1] if pt[0][0][-1].isdigit() else '5'\n",
    "                    seq.append(Phoneme(tone, 'zh', is_word_end=True))\n",
    "                except: seq.append(Phoneme(token, 'zh', is_word_start=True, is_word_end=True))\n",
    "        else: seq.append(Phoneme(token, 'zh', is_word_start=True, is_word_end=True))\n",
    "    return seq\n",
    "\n",
    "\n",
    "def get_phoneme_info(text: str, split_char: bool = True) -> List[Phoneme]:\n",
    "    if not pinyin: return [Phoneme(c, 'zh', char_start=i, char_end=i+1) for i, c in enumerate(text)]\n",
    "    seq = []; pos = 0\n",
    "    while pos < len(text):\n",
    "        char = text[pos]\n",
    "        if '\\u4e00' <= char <= '\\u9fff':\n",
    "            zh_start = pos; scan_pos = pos + 1\n",
    "            while scan_pos < len(text) and '\\u4e00' <= text[scan_pos] <= '\\u9fff': scan_pos += 1\n",
    "            zh_end = scan_pos; fragment = text[zh_start:zh_end]\n",
    "            try:\n",
    "                py_initials = pinyin(fragment, style=Style.INITIALS, strict=False)\n",
    "                py_finals = pinyin(fragment, style=Style.FINALS, strict=False)\n",
    "                py_tones = pinyin(fragment, style=Style.TONE3, neutral_tone_with_five=True)\n",
    "                min_len = min(len(fragment), len(py_initials), len(py_finals), len(py_tones))\n",
    "                for i in range(min_len):\n",
    "                    idx = zh_start + i; init, fin, tone = py_initials[i][0], py_finals[i][0], py_tones[i][0]\n",
    "                    items = []\n",
    "                    if init: items.append(Phoneme(init, 'zh', is_word_start=True, char_start=idx, char_end=idx+1))\n",
    "                    if fin: items.append(Phoneme(fin, 'zh', is_word_start=not init, char_start=idx, char_end=idx+1))\n",
    "                    if tone and tone[-1].isdigit(): items.append(Phoneme(tone[-1], 'zh', is_word_end=True, char_start=idx, char_end=idx+1))\n",
    "                    if not items: items.append(Phoneme(fragment[i], 'zh', is_word_start=True, is_word_end=True, char_start=idx, char_end=idx+1))\n",
    "                    seq.extend(items)\n",
    "            except:\n",
    "                for i, c in enumerate(fragment): seq.append(Phoneme(c, 'zh', is_word_start=True, is_word_end=True, char_start=zh_start+i, char_end=zh_start+i+1))\n",
    "            pos = zh_end\n",
    "        elif 'a' <= char.lower() <= 'z' or '0' <= char <= '9':\n",
    "            st = pos; pos += 1\n",
    "            while pos < len(text):\n",
    "                c = text[pos]\n",
    "                if not ('a' <= c.lower() <= 'z' or '0' <= c <= '9'): break\n",
    "                if (text[pos-1].islower() and c.isupper()) or (text[pos-1].isalpha() and c.isdigit()) or (text[pos-1].isdigit() and c.isalpha()): break\n",
    "                pos += 1\n",
    "            tk = text[st:pos].lower(); lang = 'num' if tk.isdigit() else 'en'\n",
    "            if split_char:\n",
    "                for i, c in enumerate(tk): seq.append(Phoneme(c, lang, is_word_start=(i==0), is_word_end=(i==len(tk)-1), char_start=st+i, char_end=st+i+1))\n",
    "            else: seq.append(Phoneme(tk, lang, is_word_start=True, is_word_end=True, char_start=st, char_end=pos))\n",
    "        else: pos += 1\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3c575",
   "metadata": {},
   "source": [
    "## 2. 相似度算法 (algo_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4b31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMILAR_PHONEMES = [{'an', 'ang'}, {'en', 'eng'}, {'in', 'ing'}, {'ian', 'iang'}, {'uan', 'uang'}, {'z', 'zh'}, {'c', 'ch'}, {'s', 'sh'}, {'l', 'n'}, {'f', 'h'}, {'ai', 'ei'}]\n",
    "\n",
    "def _lcs_length(s1: str, s2: str) -> int:\n",
    "    m, n = len(s1), len(s2)\n",
    "    if m < n: s1, s2 = s2, s1; m, n = n, m\n",
    "    if n == 0: return 0\n",
    "    prev = [0] * (n + 1); curr = [0] * (n + 1)\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            curr[j] = prev[j-1] + 1 if s1[i-1] == s2[j-1] else max(prev[j], curr[j-1])\n",
    "        prev, curr = curr, prev\n",
    "    return prev[n]\n",
    "\n",
    "def _get_tuple_cost(t1: Tuple, t2: Tuple) -> float:\n",
    "    if t1[1] != t2[1]: return 1.0\n",
    "    if t1[0] == t2[0]: return 0.0\n",
    "    if t1[1] == 'zh':\n",
    "        pair = {t1[0], t2[0]}\n",
    "        for s in SIMILAR_PHONEMES:\n",
    "            if pair.issubset(s): return 0.5\n",
    "    if t1[1] == 'en':\n",
    "        lcs = _lcs_length(t1[0], t2[0])\n",
    "        max_len = max(len(t1[0]), len(t2[0]))\n",
    "        if max_len > 0: return 1.0 - (lcs / max_len)\n",
    "    return 1.0\n",
    "\n",
    "def fuzzy_substring_distance(hw_info: List[Tuple], input_info: List[Tuple]) -> float:\n",
    "    n, m = len(hw_info), len(input_info)\n",
    "    if n == 0: return 0.0\n",
    "    if m == 0: return float(n)\n",
    "    prev = [0.0] * (m + 1); curr = [0.0] * (m + 1)\n",
    "    for i in range(1, n + 1):\n",
    "        curr[0] = float(i)\n",
    "        for j in range(1, m + 1):\n",
    "            cost = _get_tuple_cost(hw_info[i-1], input_info[j-1])\n",
    "            curr[j] = min(prev[j] + 1.0, curr[j-1] + 1.0, prev[j-1] + cost)\n",
    "        prev, curr = curr, prev\n",
    "    return min(prev)\n",
    "\n",
    "def fuzzy_substring_score(hw_info: List[Tuple], input_info: List[Tuple]) -> float:\n",
    "    n = len(hw_info)\n",
    "    if n == 0: return 0.0\n",
    "    return max(0.0, 1.0 - (fuzzy_substring_distance(hw_info, input_info) / n))\n",
    "\n",
    "def fuzzy_substring_search_constrained(hw_info: List[Tuple], input_info: List[Tuple], threshold: float = 0.6) -> List[Tuple[float, int, int]]:\n",
    "    n, m = len(hw_info), len(input_info)\n",
    "    if n == 0 or m == 0: return []\n",
    "    dp = [[float('inf')] * (m + 1) for _ in range(n + 1)]\n",
    "    path = [[(0, 0)] * (m + 1) for _ in range(n + 1)]\n",
    "    for j in range(m + 1):\n",
    "        if j == 0 or (j < m and input_info[j][2]): dp[0][j] = 0.0; path[0][j] = (0, j)\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = _get_tuple_cost(hw_info[i-1], input_info[j-1])\n",
    "            dist_match = dp[i-1][j-1] + cost\n",
    "            dist_del = dp[i-1][j] + 1.0\n",
    "            dist_ins = dp[i][j-1] + 1.0\n",
    "            min_dist = min(dist_match, dist_del, dist_ins)\n",
    "            dp[i][j] = min_dist\n",
    "            if min_dist == dist_match: path[i][j] = path[i-1][j-1]\n",
    "            elif min_dist == dist_del: path[i][j] = path[i-1][j]\n",
    "            else: path[i][j] = path[i][j-1]\n",
    "    results = []\n",
    "    for j in range(1, m + 1):\n",
    "        if not input_info[j-1][3]: continue\n",
    "        dist = dp[n][j]\n",
    "        if dist >= n * 0.8: continue\n",
    "        score = 1.0 - (dist / n)\n",
    "        if score >= threshold: results.append((score, path[n][j][1], j))\n",
    "    results.sort(key=lambda x: x[0], reverse=True)\n",
    "    used_ends = {}\n",
    "    for score, s, e in results:\n",
    "        if e not in used_ends or score > used_ends[e][0]: used_ends[e] = (score, s, e)\n",
    "    return sorted(used_ends.values(), key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee7e0b",
   "metadata": {},
   "source": [
    "## 3. RAG 加速检索与核心算法 (rag_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95cbd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_NUMBA and HAS_NUMPY:\n",
    "    @njit(cache=True)\n",
    "    def _fuzzy_substring_numba(main_codes: np.ndarray, sub_codes: np.ndarray) -> float:\n",
    "        n, m = len(sub_codes), len(main_codes)\n",
    "        if n == 0 or m == 0: return float(n)\n",
    "        dp = np.zeros((n + 1, m + 1), dtype=np.float32)\n",
    "        for i in range(1, n + 1): dp[i, 0] = float(i)\n",
    "        for i in range(1, n + 1):\n",
    "            for j in range(1, m + 1):\n",
    "                cost = 0.0 if sub_codes[i-1] == main_codes[j-1] else 1.0\n",
    "                dp[i, j] = min(dp[i-1, j] + 1.0, dp[i, j-1] + 1.0, dp[i-1, j-1] + cost)\n",
    "        return float(np.min(dp[n, 1:]))\n",
    "\n",
    "class FastRAG:\n",
    "    def __init__(self, threshold: float = 0.6):\n",
    "        self.threshold = threshold\n",
    "        self.ph_to_code = {}; self.next_code = 1\n",
    "        self.index = defaultdict(list); self.hotword_count = 0\n",
    "    def _encode(self, p: str) -> int:\n",
    "        if p not in self.ph_to_code: self.ph_to_code[p] = self.next_code; self.next_code += 1\n",
    "        return self.ph_to_code[p]\n",
    "    def _encode_seq(self, phs: List[str]) -> np.ndarray:\n",
    "        return np.array([self._encode(p) for p in phs], dtype=np.int32) if HAS_NUMPY else [self._encode(p) for p in phs]\n",
    "    def add_hotwords(self, hotwords: Dict[str, List[Phoneme]]):\n",
    "        for hw, phs in hotwords.items():\n",
    "            if not phs: continue\n",
    "            codes = self._encode_seq([p.value for p in phs])\n",
    "            indices = [0]\n",
    "            if phs[0].lang == 'en': indices = list(range(min(len(codes), 2)))\n",
    "            for i in indices: self.index[codes[i]].append((hw, codes))\n",
    "            self.hotword_count += 1\n",
    "    def search(self, input_phs: List[Phoneme], top_k: int = 10) -> List[Tuple[str, float]]:\n",
    "        if not input_phs: return []\n",
    "        input_codes = self._encode_seq([p.value for p in input_phs]); unique = set(input_codes)\n",
    "        candidates = []\n",
    "        seen = set()\n",
    "        for c in unique:\n",
    "            for hw, codes in self.index.get(c, []):\n",
    "                if hw not in seen: candidates.append((hw, codes)); seen.add(hw)\n",
    "        results = []\n",
    "        for hw, h_codes in candidates:\n",
    "            if len(h_codes) > len(input_codes) + 3: continue\n",
    "            if HAS_NUMBA and HAS_NUMPY: dist = _fuzzy_substring_numba(input_codes, h_codes)\n",
    "            else: dist = self._python_dist(input_codes, h_codes)\n",
    "            score = 1.0 - (dist / len(h_codes))\n",
    "            if score >= self.threshold: results.append((hw, round(score, 3)))\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results[:top_k]\n",
    "    def _python_dist(self, main, sub):\n",
    "        n, m = len(sub), len(main)\n",
    "        dp = [[0.0] * (m + 1) for _ in range(n + 1)]\n",
    "        for i in range(1, n + 1): dp[i][0] = float(i)\n",
    "        for i in range(1, n + 1):\n",
    "            for j in range(1, m + 1):\n",
    "                cost = 0.0 if sub[i-1] == main[j-1] else 1.0\n",
    "                dp[i][j] = min(dp[i-1][j] + 1.0, dp[i][j-1] + 1.0, dp[i-1][j-1] + cost)\n",
    "        return min(dp[n][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90111570",
   "metadata": {},
   "source": [
    "## 4. 纠错系统逻辑 (hot_phoneme & hot_rectification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "470d32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchResult(NamedTuple):\n",
    "    start: int; end: int; score: float; hotword: str\n",
    "\n",
    "class CorrectionResult(NamedTuple):\n",
    "    text: str; matchs: List[Tuple[str, str, float]]; similars: List[Tuple[str, str, float]]\n",
    "\n",
    "class PhonemeCorrector:\n",
    "    def __init__(self, threshold: float = 0.7, similar_threshold: float = None):\n",
    "        self.threshold = threshold\n",
    "        self.similar_threshold = similar_threshold if similar_threshold is not None else threshold - 0.2\n",
    "        self.hotwords: Dict[str, List[Phoneme]] = {}\n",
    "        self.fast_rag = FastRAG(threshold=min(self.threshold, self.similar_threshold) - 0.1)\n",
    "        self._lock = threading.Lock()\n",
    "    def update_hotwords(self, text: str) -> int:\n",
    "        lines = [l.strip() for l in text.splitlines() if l.strip() and not l.strip().startswith('#')]\n",
    "        new_hw = {}\n",
    "        for hw in lines:\n",
    "            phs = get_phoneme_info(hw)\n",
    "            if phs: new_hw[hw] = phs\n",
    "        with self._lock:\n",
    "            self.hotwords = new_hw\n",
    "            self.fast_rag = FastRAG(threshold=min(self.threshold, self.similar_threshold) - 0.1)\n",
    "            self.fast_rag.add_hotwords(new_hw)\n",
    "        return len(new_hw)\n",
    "    def load_hotwords_file(self, path: str) -> int:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'r', encoding='utf-8') as f: return self.update_hotwords(f.read())\n",
    "        return 0\n",
    "    def _find_matches(self, text, fast_results, input_processed):\n",
    "        matches, similars = [], []\n",
    "        search_thresh = min(self.threshold, self.similar_threshold) - 0.1\n",
    "        for hw, _ in fast_results:\n",
    "            hw_phs = self.hotwords[hw]; hw_compare = [p.info[:5] for p in hw_phs]\n",
    "            found = fuzzy_substring_search_constrained(hw_compare, input_processed, threshold=search_thresh)\n",
    "            for score, s_idx, e_idx in found:\n",
    "                char_st, char_ed = input_processed[s_idx][5], input_processed[e_idx-1][6]\n",
    "                res = MatchResult(char_st, char_ed, score, hw)\n",
    "                if score >= self.threshold: matches.append(res)\n",
    "                if score >= self.similar_threshold: similars.append((text[char_st:char_ed], hw, score))\n",
    "        similars.sort(key=lambda x: (x[2], len(x[1])), reverse=True)\n",
    "        final_sims, seen = [], set()\n",
    "        for o, hw, s in similars:\n",
    "            if hw not in seen: final_sims.append((o, hw, s)); seen.add(hw)\n",
    "        return matches, final_sims\n",
    "    def _resolve_and_replace(self, text, matches):\n",
    "        matches.sort(key=lambda x: (x.score, x.end - x.start), reverse=True)\n",
    "        final_m, occupied = [], []\n",
    "        for m in matches:\n",
    "            if any(not (m.end <= rs or m.start >= re) for rs, re in occupied): continue\n",
    "            if text[m.start:m.end] != m.hotword: final_m.append(m)\n",
    "            occupied.append((m.start, m.end))\n",
    "        res = list(text); final_m.sort(key=lambda x: x.start, reverse=True)\n",
    "        for m in final_m: res[m.start:m.end] = list(m.hotword)\n",
    "        return \"\".join(res), [(text[m.start:m.end], m.hotword, m.score) for m in sorted(final_m, key=lambda x: x.start)]\n",
    "    def correct(self, text, k=10):\n",
    "        in_phs = get_phoneme_info(text)\n",
    "        if not in_phs or not self.hotwords: return CorrectionResult(text, [], [])\n",
    "        with self._lock:\n",
    "            fast_res = self.fast_rag.search(in_phs, top_k=100); processed = [p.info for p in in_phs]\n",
    "            matches, sims = self._find_matches(text, fast_res, processed)\n",
    "        nt, fhw = self._resolve_and_replace(text, matches)\n",
    "        return CorrectionResult(nt, fhw, sims[:k])\n",
    "\n",
    "def _get_word_boundaries(text: str) -> List[Tuple[int, int, str]]:\n",
    "    bounds, i, n = [], 0, len(text)\n",
    "    while i < n:\n",
    "        if not (text[i].isalnum() or '\\u4e00' <= text[i] <= '\\u9fff'): i += 1; continue\n",
    "        s = i\n",
    "        if '\\u4e00' <= text[i] <= '\\u9fff': i += 1\n",
    "        else:\n",
    "            low = text[i].islower()\n",
    "            while i < n and text[i].isalnum():\n",
    "                if text[i].isupper() and low and i > s: break\n",
    "                low = text[i].islower(); i += 1\n",
    "        bounds.append((s, i, text[s:i]))\n",
    "    return bounds\n",
    "\n",
    "def extract_diff_fragments(wrong: str, right: str) -> List[str]:\n",
    "    wb, rb = _get_word_boundaries(wrong), _get_word_boundaries(right)\n",
    "    matcher = SequenceMatcher(None, [b[2] for b in wb], [b[2] for b in rb]); frags = []\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag in ('replace', 'delete') and i2 > i1: frags.append(wrong[wb[i1][0]:wb[i2-1][1]])\n",
    "        if tag in ('replace', 'insert') and j2 > j1: frags.append(right[rb[j1][0]:rb[j2-1][1]])\n",
    "    return list(dict.fromkeys(frags))\n",
    "\n",
    "class RectificationRAG:\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold = threshold; self.records = []; self._lock = threading.Lock()\n",
    "    def load_rectify_text(self, text):\n",
    "        recs = []\n",
    "        for block in text.split('---'):\n",
    "            lines = [l.strip() for l in block.split('\\n') if l.strip() and not l.strip().startswith('#')]\n",
    "            if len(lines) >= 2:\n",
    "                w, r = lines[0], lines[1]; frags = extract_diff_fragments(w, r) or [w]\n",
    "                recs.append({'wrong': w, 'right': r, 'fphs': {f: [p.info[:5] for p in get_phoneme_seq(f)] for f in frags}})\n",
    "        with self._lock: self.records = recs\n",
    "    def load_rectify_file(self, path: str):\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'r', encoding='utf-8') as f: self.load_rectify_text(f.read())\n",
    "    def search(self, text, top_k=5):\n",
    "        in_phs = [p.info[:5] for p in get_phoneme_seq(text)]; matches = []\n",
    "        with self._lock:\n",
    "            for rec in self.records:\n",
    "                best = 0.0\n",
    "                for fphs in rec['fphs'].values():\n",
    "                    if not fphs: continue\n",
    "                    score = fuzzy_substring_score(fphs, in_phs)\n",
    "                    if score > best: best = score\n",
    "                if best >= self.threshold: matches.append((rec['wrong'], rec['right'], round(best, 3)))\n",
    "        return sorted(matches, key=lambda x: x[2], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8635fd",
   "metadata": {},
   "source": [
    "## 5. 调试工具 (Phoneme Debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a83e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phoneme_cost(p1: Phoneme, p2: Phoneme) -> float:\n",
    "    if p1.lang != p2.lang: return 1.0\n",
    "    if p1.value == p2.value: return 0.0\n",
    "    if p1.lang == 'zh' and p2.lang == 'zh':\n",
    "        pair = {p1.value, p2.value}\n",
    "        for s in SIMILAR_PHONEMES:\n",
    "            if pair.issubset(s): return 0.5\n",
    "    if p1.lang == 'en' and p2.lang == 'en':\n",
    "        lcs_len = _lcs_length(p1.value, p2.value)\n",
    "        max_len = max(len(p1.value), len(p2.value))\n",
    "        return 1.0 - (lcs_len / max_len)\n",
    "    return 1.0\n",
    "\n",
    "def find_best_match(main_seq: List[Phoneme], sub_seq: List[Phoneme]) -> Tuple[float, int, int]:\n",
    "    n, m = len(sub_seq), len(main_seq)\n",
    "    if n == 0 or m == 0: return 0.0, 0, 0\n",
    "    valid_starts = [j for j in range(m) if main_seq[j].is_word_start]\n",
    "    dp = [[0.0] * (m + 1) for _ in range(n + 1)]\n",
    "    for j in range(m + 1):\n",
    "        if j not in valid_starts: dp[0][j] = float('inf')\n",
    "    for i in range(1, n + 1): dp[i][0] = dp[i-1][0] + 1.0\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = get_phoneme_cost(sub_seq[i-1], main_seq[j-1])\n",
    "            dp[i][j] = min(dp[i-1][j] + 1.0, dp[i][j-1] + 1.0, dp[i-1][j-1] + cost)\n",
    "    min_dist, end_pos, best_start = float('inf'), 0, 0\n",
    "    for j in range(1, m + 1):\n",
    "        if dp[n][j] < min_dist:\n",
    "            curr_i, curr_j = n, j\n",
    "            while curr_i > 0:\n",
    "                cost = get_phoneme_cost(sub_seq[curr_i-1], main_seq[curr_j-1])\n",
    "                if curr_j > 0 and abs(dp[curr_i][curr_j] - (dp[curr_i-1][curr_j-1] + cost)) < 1e-9:\n",
    "                    curr_i -= 1; curr_j -= 1\n",
    "                elif abs(dp[curr_i][curr_j] - (dp[curr_i-1][curr_j] + 1.0)) < 1e-9: curr_i -= 1\n",
    "                elif curr_j > 0 and abs(dp[curr_i][curr_j] - (dp[curr_i][curr_j-1] + 1.0)) < 1e-9: curr_j -= 1\n",
    "                else: curr_i -= 1\n",
    "            if curr_j in valid_starts:\n",
    "                min_dist = dp[n][j]; end_pos = j; best_start = curr_j\n",
    "    return 1.0 - (min_dist / n), best_start, end_pos\n",
    "\n",
    "def test_pair(input_text, hotword, split_char=True):\n",
    "    print(f\"--- Testing: '{input_text}' vs '{hotword}' ---\")\n",
    "    input_seq = get_phoneme_info(input_text, split_char=split_char)\n",
    "    target_seq = get_phoneme_info(hotword, split_char=split_char)\n",
    "    print(f\"Input Seq: {[p.value for p in input_seq]}\")\n",
    "    print(f\"Target Seq: {[p.value for p in target_seq]}\")\n",
    "    score, start, end = find_best_match(input_seq, target_seq)\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    if score > 0:\n",
    "        matched_segment = input_seq[start:end]\n",
    "        print(f\"Matched Segment: {[p.value for p in matched_segment]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1706ac",
   "metadata": {},
   "source": [
    "## 6. LLM 集成 (Prompt Builder & Ollama Client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1988fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptBuilder:\n",
    "    def __init__(self, system_prompt: str = \"你是一个输入法纠错助。\"):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.prompt_prefix_hotwords = \"热词列表：\"\n",
    "        self.prompt_prefix_rectify = \"纠错历史：\\n\"\n",
    "        self.prompt_prefix_input = \"用户输入：\"\n",
    "\n",
    "    def build(self, user_content: str, hotwords: List[Tuple[str, float]] = None, rectify_matches: List[Tuple[str, str, float]] = None) -> List[Dict]:\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        context_parts = []\n",
    "        if hotwords:\n",
    "            words = [hw for hw, _, _ in hotwords]\n",
    "            context_parts.append(f\"{self.prompt_prefix_hotwords}[{', '.join(words)}]\")\n",
    "        if rectify_matches:\n",
    "            lines = [self.prompt_prefix_rectify]\n",
    "            for wrong, right, _ in rectify_matches: lines.append(f\"- {wrong} => {right}\")\n",
    "            context_parts.append(\"\\n\".join(lines))\n",
    "        context_str = \"\\n\\n\".join(context_parts)\n",
    "        full_user_content = f\"{context_str}\\n\\n{self.prompt_prefix_input}{user_content}\"\n",
    "        messages.append({\"role\": \"user\", \"content\": full_user_content})\n",
    "        return messages\n",
    "\n",
    "def ollama_chat(messages: List[Dict], model: str = \"gemma3:4b\", stream: bool = True):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    payload = {\"model\": model, \"messages\": messages, \"stream\": stream}\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, stream=stream)\n",
    "        if not stream: return response.json().get('message', {}).get('content', '')\n",
    "        full_res = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                chunk = json.loads(line)\n",
    "                content = chunk.get('message', {}).get('content', '')\n",
    "                full_res += content; print(content, end=\"\", flush=True)\n",
    "                if chunk.get('done'): break\n",
    "        print(); return full_res\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Error calling Ollama]: {e}\"); return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb487a",
   "metadata": {},
   "source": [
    "## 7. 综合数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728a1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotwords_data = \"\"\"\n",
    "Claude\n",
    "Bilibili\n",
    "Microsoft\n",
    "麦当劳\n",
    "肯德基\n",
    "VsCode\n",
    "七浦路\n",
    "\"\"\"\n",
    "\n",
    "rectify_data = \"\"\"\n",
    "把那个锯子给我\n",
    "把那个句子给我\n",
    "---\n",
    "cloud code is good\n",
    "Claude Code is good\n",
    "\"\"\"\n",
    "\n",
    "cases = [\n",
    "    \"我想去吃买当劳和肯得鸡\",\n",
    "    \"喜欢刷Bili Bili\",\n",
    "    \"请把那个锯子发给我一下\",\n",
    "    \"我很喜欢 cloud\",\n",
    "    \"西安是一个好地方\",\n",
    "    \"我刚才先了一下\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05b4bd",
   "metadata": {},
   "source": [
    "## 8. 系统初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff9ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrector = PhonemeCorrector(threshold=0.8)\n",
    "rectifier = RectificationRAG(threshold=0.5)\n",
    "\n",
    "# 加载演示数据 (也可通过 load_hotwords_file / load_rectify_file 加载外部文件)\n",
    "corrector.update_hotwords(hotwords_data)\n",
    "rectifier.load_rectify_text(rectify_data)\n",
    "\n",
    "# 尝试加载外部文件 (如果存在)\n",
    "corrector.load_hotwords_file(\"hot.txt\")\n",
    "rectifier.load_rectify_file(\"hot-rectify.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d0723",
   "metadata": {},
   "source": [
    "## 9. 执行综合纠错与 RAG 演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9c486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "【 CapsWriter-Offline 综合纠错系统演示 】\n",
      "==================================================\n",
      "\n",
      "\n",
      "Case 1: '我想去吃买当劳和肯得鸡'\n",
      "  [纠错结果] 我想去吃麦当劳和肯德基\n",
      "  [匹配热词]\n",
      "    - (0.889) 买当劳 => 麦当劳 \n",
      "    - (1.000) 肯得鸡 => 肯德基 \n",
      "  [潜在热词]\n",
      "    - (1.000) 肯得鸡 => 肯德基 \n",
      "    - (0.889) 买当劳 => 麦当劳 \n",
      "  [历史纠错]\n",
      "    - (0.667) 把那个锯子给我 => 把那个句子给我 \n",
      "\n",
      "\n",
      "Case 2: '喜欢刷Bili Bili'\n",
      "  [纠错结果] 喜欢刷Bilibili\n",
      "  [匹配热词]\n",
      "    - (1.000) Bili Bili => Bilibili \n",
      "  [潜在热词]\n",
      "    - (1.000) Bili Bili => Bilibili \n",
      "\n",
      "\n",
      "Case 3: '请把那个锯子发给我一下'\n",
      "  [纠错结果] 请把那个锯子发给我一下\n",
      "  [历史纠错]\n",
      "    - (1.000) 把那个锯子给我 => 把那个句子给我 \n",
      "\n",
      "\n",
      "Case 4: '我很喜欢 cloud'\n",
      "  [纠错结果] 我很喜欢 cloud\n",
      "  [潜在热词]\n",
      "    - (0.667) cloud => Claude \n",
      "  [历史纠错]\n",
      "    - (0.500) cloud code is good => Claude Code is good \n",
      "\n",
      "\n",
      "Case 5: '西安是一个好地方'\n",
      "  [纠错结果] 西安是一个好地方\n",
      "\n",
      "\n",
      "Case 6: '我刚才先了一下'\n",
      "  [纠错结果] 我刚才先了一下\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"【 CapsWriter-Offline 综合纠错系统演示 】\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, t in enumerate(cases):\n",
    "    print(f\"\\n\\nCase {i+1}: '{t}'\")\n",
    "    result = corrector.correct(t)\n",
    "    print(f\"  [纠错结果] {result.text}\")\n",
    "    if result.matchs: \n",
    "        print(f\"  [匹配热词]\")\n",
    "        print(\"\\n\".join([f\"    - ({score:.3f}) {wrong} => {right} \" for wrong, right, score in result.matchs]))\n",
    "    if result.similars: \n",
    "        print(f\"  [潜在热词]\")\n",
    "        print(\"\\n\".join([f\"    - ({score:.3f}) {wrong} => {right} \" for wrong, right, score in result.similars]))\n",
    "    rag_results = rectifier.search(t)\n",
    "    if rag_results:\n",
    "        print(f\"  [历史纠错]\")\n",
    "        print(\"\\n\".join([f\"    - ({score:.3f}) {wrong} => {right} \" for wrong, right, score in rag_results]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901627e",
   "metadata": {},
   "source": [
    "## 10. 音素匹配调试演示 (test_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6163670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "【 Phoneme Debug 调试演示 】\n",
      "==================================================\n",
      "--- Testing: 'cloud' vs 'claude' ---\n",
      "Input Seq: ['c', 'l', 'o', 'u', 'd']\n",
      "Target Seq: ['c', 'l', 'a', 'u', 'd', 'e']\n",
      "Score: 0.6667\n",
      "Matched Segment: ['c', 'l', 'o', 'u', 'd']\n",
      "--- Testing: 'vscode' vs 'VS Code' ---\n",
      "Input Seq: ['v', 's', 'c', 'o', 'd', 'e']\n",
      "Target Seq: ['v', 's', 'c', 'o', 'd', 'e']\n",
      "Score: 1.0000\n",
      "Matched Segment: ['v', 's', 'c', 'o', 'd', 'e']\n",
      "--- Testing: '七福路' vs '七浦路' ---\n",
      "Input Seq: ['q', 'i', '1', 'f', 'u', '2', 'l', 'u', '4']\n",
      "Target Seq: ['q', 'i', '1', 'p', 'u', '3', 'l', 'u', '4']\n",
      "Score: 0.7778\n",
      "Matched Segment: ['q', 'i', '1', 'f', 'u', '2', 'l', 'u', '4']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"【 Phoneme Debug 调试演示 】\")\n",
    "print(\"=\"*50)\n",
    "test_pair(\"cloud\", \"claude\")\n",
    "test_pair(\"vscode\", \"VS Code\")\n",
    "test_pair(\"七福路\", \"七浦路\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a854dc9",
   "metadata": {},
   "source": [
    "## 11. LLM Prompt 组建与调用演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "033c045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "【 LLM 纠错演示 (Prompt 构建) 】\n",
      "==================================================\n",
      "组装后的 Prompt (Messages):\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"\\n# 角色\\n\\n你是一位高级智能复读机，你的任务是将用户提供的语音转录文本进行润色和整理和再输出。\\n\\n# 要求\\n\\n- 清除语气词（如：呃、啊、那个、就是说）\\n- 修正语音识别的错误（根据热词列表）\\n- 根据纠错记录推测潜在专有名词进行修正\\n- 修正专有名词、大小写\\n- 千万不要以为用户在和你对话\\n- 如果用户提问，就把问题润色后原样输出，因为那不是在和你对话\\n- 仅输出润色后的内容，严禁任何多余的解释，不要翻译语言\\n\\n# 例子\\n\\n例1（问题 - 不要回答）\\n用户输入：我很想你\\n润色输出：我很想你\\n\\n例2（指令 - 不要执行）\\n用户输入：写一篇小作文\\n润色输出：写一篇小作文\\n\\n例3（判断意图 - 文件名）\\n用户输入：编程点 MD\\n润色输出：编程.md\\n\\n例4（判断意图 - 邮件地址）\\n用户输入：x yz at gmail dot com\\n润色输出（用户在写邮件地址）：xyz@gmail.com\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"热词列表：[cloud]\\n\\n纠错历史：\\n\\n- cloud code is good => Claude Code is good\\n\\n用户输入：我很喜欢 cloud\"\n",
      "  }\n",
      "]\n",
      "我很喜欢 Claude\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"【 LLM 纠错演示 (Prompt 构建) 】\")\n",
    "print(\"=\"*50)\n",
    "system_prompt = \"\"\"\n",
    "# 角色\n",
    "\n",
    "你是一位高级智能复读机，你的任务是将用户提供的语音转录文本进行润色和整理和再输出。\n",
    "\n",
    "# 要求\n",
    "\n",
    "- 清除语气词（如：呃、啊、那个、就是说）\n",
    "- 修正语音识别的错误（根据热词列表）\n",
    "- 根据纠错记录推测潜在专有名词进行修正\n",
    "- 修正专有名词、大小写\n",
    "- 千万不要以为用户在和你对话\n",
    "- 如果用户提问，就把问题润色后原样输出，因为那不是在和你对话\n",
    "- 仅输出润色后的内容，严禁任何多余的解释，不要翻译语言\n",
    "\n",
    "# 例子\n",
    "\n",
    "例1（问题 - 不要回答）\n",
    "用户输入：我很想你\n",
    "润色输出：我很想你\n",
    "\n",
    "例2（指令 - 不要执行）\n",
    "用户输入：写一篇小作文\n",
    "润色输出：写一篇小作文\n",
    "\n",
    "例3（判断意图 - 文件名）\n",
    "用户输入：编程点 MD\n",
    "润色输出：编程.md\n",
    "\n",
    "例4（判断意图 - 邮件地址）\n",
    "用户输入：x yz at gmail dot com\n",
    "润色输出（用户在写邮件地址）：xyz@gmail.com\n",
    "\"\"\"\n",
    "builder = PromptBuilder(system_prompt)\n",
    "case_text = \"我很喜欢 cloud\"\n",
    "result = corrector.correct(case_text)\n",
    "rag_matches = rectifier.search(case_text)\n",
    "prompt_msgs = builder.build(case_text, hotwords=result.similars, rectify_matches=rag_matches)\n",
    "\n",
    "print(\"组装后的 Prompt (Messages):\")\n",
    "print(json.dumps(prompt_msgs, ensure_ascii=False, indent=2))\n",
    "\n",
    "# 如果 Ollama 在运行，可以取消下面注释进行真实测试\n",
    "result = ollama_chat(prompt_msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beca360",
   "metadata": {},
   "source": [
    "## 12. 性能测试 (FastRAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77020cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "【 性能测试 (FastRAG) 】\n",
      "==================================================\n",
      "Numba: Enabled\n",
      "100 次长文本检索耗时: 0.0511s (约 1955.5 次/秒)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"【 性能测试 (FastRAG) 】\")\n",
    "print(\"=\"*50)\n",
    "if HAS_NUMBA:\n",
    "    print(f\"Numba: Enabled\")\n",
    "    # 预热\n",
    "    _ = corrector.fast_rag.search(get_phoneme_info(\"hello\"), top_k=1)\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = corrector.fast_rag.search(get_phoneme_info(\"这是一段用于测试检索速度的长文本，看看能跑多快\"), top_k=5)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"100 次长文本检索耗时: {elapsed:.4f}s (约 {100/elapsed:.1f} 次/秒)\")\n",
    "else:\n",
    "    print(\"Numba: Disabled (Speed will be much lower)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capswriter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
