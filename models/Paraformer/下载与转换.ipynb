{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13dc6eb9",
   "metadata": {},
   "source": [
    "本 Notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd8f9e",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install funasr torch onnxruntime numpy modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273dc3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download(\n",
    "    model_id='iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch', \n",
    "    local_dir='speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch'\n",
    ")\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6880d8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "开始导出 Paraformer 模型到 ONNX 格式\n",
      "================================================================================\n",
      "\n",
      "模型名称: ./speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\n",
      "量化模式: 否\n",
      "ONNX Opset 版本: 14\n",
      "\n",
      "正在应用导出补丁...\n",
      "✓ 已应用 ONNX 导出补丁（使用旧版导出器）\n",
      "\n",
      "正在加载模型...\n",
      "funasr version: 1.3.0.\n",
      "Check update of funasr, and it would cost few times. You may disable it by set `disable_update=True` in AutoModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:trust_remote_code: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the latest version of funasr-1.3.0\n",
      "✓ 模型加载成功！\n",
      "\n",
      "开始导出 ONNX 模型...\n",
      "\n",
      "使用旧版 ONNX 导出器...\n",
      "导出路径: ./speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\\model.onnx\n",
      "\n",
      "================================================================================\n",
      "✓ 导出成功！\n",
      "================================================================================\n",
      "\n",
      "ONNX 模型保存路径: ./speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch\n",
      "\n",
      "导出的文件列表:\n",
      "  - .DS_Store                                      0.01 MB\n",
      "  - .mdl                                           0.00 MB\n",
      "  - .msc                                           0.00 MB\n",
      "  - .mv                                            0.00 MB\n",
      "  - README.md                                      0.02 MB\n",
      "  - am.mvn                                         0.01 MB\n",
      "  - config.yaml                                    0.00 MB\n",
      "  - configuration.json                             0.00 MB\n",
      "  - model.onnx                                   843.91 MB\n",
      "  - model.pt                                     858.98 MB\n",
      "  - seg_dict                                       7.90 MB\n",
      "  - tokens.json                                    0.09 MB\n",
      "  - tokens.txt                                     0.08 MB\n",
      "\n",
      "  总大小: 1711.00 MB\n",
      "\n",
      "================================================================================\n",
      "提示:\n",
      "  1. 如需量化模型，请将 EXPORT_CONFIG['quantize'] 设置为 True\n",
      "  2. 量化后的模型文件名为 model_quant.onnx\n",
      "  3. 非量化模型文件名为 model.onnx\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "将 Paraformer 模型导出为 ONNX 格式\n",
    "使用旧版 ONNX 导出器绕过 torch.export 的兼容性问题\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from funasr import AutoModel\n",
    "\n",
    "def patch_export_utils():\n",
    "    \"\"\"\n",
    "    修补 export_utils 以使用旧版 ONNX 导出器\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from funasr.utils import export_utils\n",
    "    \n",
    "    # 保存原始的 _onnx 函数\n",
    "    original_onnx = export_utils._onnx\n",
    "    \n",
    "    def patched_onnx(\n",
    "        model,\n",
    "        data_in=None,\n",
    "        quantize: bool = False,\n",
    "        opset_version: int = 14,\n",
    "        export_dir: str = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"修补后的 ONNX 导出函数，使用旧版导出器\"\"\"\n",
    "        device = kwargs.get(\"device\", \"cpu\")\n",
    "        dummy_input = model.export_dummy_inputs()\n",
    "\n",
    "        if isinstance(dummy_input, torch.Tensor):\n",
    "            dummy_input = dummy_input.to(device)\n",
    "        else:\n",
    "            dummy_input = tuple([input.to(device) for input in dummy_input])\n",
    "\n",
    "        verbose = kwargs.get(\"verbose\", False)\n",
    "\n",
    "        if isinstance(model.export_name, str):\n",
    "            export_name = model.export_name + \".onnx\"\n",
    "        else:\n",
    "            export_name = model.export_name()\n",
    "        model_path = os.path.join(export_dir, export_name)\n",
    "        \n",
    "        print(f\"\\n使用旧版 ONNX 导出器...\")\n",
    "        print(f\"导出路径: {model_path}\")\n",
    "        \n",
    "        # 使用旧版导出器，避免 torch.export 的问题\n",
    "        with torch.onnx.select_model_mode_for_export(model, torch.onnx.TrainingMode.EVAL):\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                dummy_input,\n",
    "                model_path,\n",
    "                verbose=verbose,\n",
    "                do_constant_folding=True,\n",
    "                opset_version=opset_version,\n",
    "                input_names=model.export_input_names(),\n",
    "                output_names=model.export_output_names(),\n",
    "                dynamic_axes=model.export_dynamic_axes(),\n",
    "                export_params=True,\n",
    "                # 关键：使用旧版导出器\n",
    "                dynamo=False,  # 禁用 dynamo（新版导出器）\n",
    "            )\n",
    "\n",
    "        if quantize:\n",
    "            try:\n",
    "                from onnxruntime.quantization import QuantType, quantize_dynamic\n",
    "                import onnx\n",
    "            except:\n",
    "                raise RuntimeError(\n",
    "                    \"You are quantizing the onnx model, please install onnxruntime first. via \\n`pip install onnx`\\n`pip install onnxruntime`.\"\n",
    "                )\n",
    "\n",
    "            quant_model_path = model_path.replace(\".onnx\", \"_quant.onnx\")\n",
    "            onnx_model = onnx.load(model_path)\n",
    "            nodes = [n.name for n in onnx_model.graph.node]\n",
    "            nodes_to_exclude = [\n",
    "                m for m in nodes if \"output\" in m or \"bias_encoder\" in m or \"bias_decoder\" in m\n",
    "            ]\n",
    "            print(\"Quantizing model from {} to {}\".format(model_path, quant_model_path))\n",
    "            quantize_dynamic(\n",
    "                model_input=model_path,\n",
    "                model_output=quant_model_path,\n",
    "                op_types_to_quantize=[\"MatMul\"],\n",
    "                per_channel=True,\n",
    "                reduce_range=False,\n",
    "                weight_type=QuantType.QUInt8,\n",
    "                nodes_to_exclude=nodes_to_exclude,\n",
    "            )\n",
    "    \n",
    "    # 替换导出函数\n",
    "    export_utils._onnx = patched_onnx\n",
    "    print(\"✓ 已应用 ONNX 导出补丁（使用旧版导出器）\")\n",
    "\n",
    "\n",
    "def export_paraformer_to_onnx(MODEL_NAME, EXPORT_CONFIG):\n",
    "    \"\"\"\n",
    "    导出 Paraformer 模型到 ONNX 格式\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"开始导出 Paraformer 模型到 ONNX 格式\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n模型名称: {MODEL_NAME}\")\n",
    "    print(f\"量化模式: {'是' if EXPORT_CONFIG['quantize'] else '否'}\")\n",
    "    print(f\"ONNX Opset 版本: {EXPORT_CONFIG['opset_version']}\")\n",
    "    \n",
    "    # 应用补丁\n",
    "    print(\"\\n正在应用导出补丁...\")\n",
    "    patch_export_utils()\n",
    "    \n",
    "    print(\"\\n正在加载模型...\")\n",
    "    \n",
    "    # 加载模型\n",
    "    model = AutoModel(\n",
    "        model=MODEL_NAME,\n",
    "        device=EXPORT_CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    print(\"✓ 模型加载成功！\")\n",
    "    print(\"\\n开始导出 ONNX 模型...\")\n",
    "    \n",
    "    # 导出模型\n",
    "    try:\n",
    "        export_dir = model.export(\n",
    "            type='onnx',\n",
    "            quantize=EXPORT_CONFIG['quantize'],\n",
    "            opset_version=EXPORT_CONFIG['opset_version'],\n",
    "            fallback_num=EXPORT_CONFIG['fallback_num'],\n",
    "            calib_num=EXPORT_CONFIG['calib_num'],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n导出过程中遇到错误，尝试使用备用方案...\")\n",
    "        print(f\"错误信息: {str(e)[:200]}...\")\n",
    "        raise\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ 导出成功！\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nONNX 模型保存路径: {export_dir}\")\n",
    "    \n",
    "    # 显示导出的文件\n",
    "    print(\"\\n导出的文件列表:\")\n",
    "    if os.path.exists(export_dir):\n",
    "        total_size = 0\n",
    "        for file in sorted(os.listdir(export_dir)):\n",
    "            file_path = os.path.join(export_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                total_size += size_mb\n",
    "                print(f\"  - {file:<40} {size_mb:>10.2f} MB\")\n",
    "        print(f\"\\n  总大小: {total_size:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"提示:\")\n",
    "    print(\"  1. 如需量化模型，请将 EXPORT_CONFIG['quantize'] 设置为 True\")\n",
    "    print(\"  2. 量化后的模型文件名为 model_quant.onnx\")\n",
    "    print(\"  3. 非量化模型文件名为 model.onnx\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return export_dir\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 模型配置\n",
    "MODEL_NAME = './speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch'\n",
    "\n",
    "# 导出配置\n",
    "EXPORT_CONFIG = {\n",
    "    'quantize': False,          # 是否量化，True 为 int8 量化，False 为 fp32\n",
    "    'device': 'cpu',            # 导出时使用的设备\n",
    "    'opset_version': 14,        # ONNX opset \n",
    "    'fallback_num': 5,          # 量化时的 fallback 数量\n",
    "    'calib_num': 100,           # 量化时的校准数量\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    export_dir = export_paraformer_to_onnx(MODEL_NAME, EXPORT_CONFIG)\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✗ 导出失败！\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"错误信息: {e}\")\n",
    "    print(\"\\n请确保:\")\n",
    "    print(\"  1. 已安装 funasr: pip install -U funasr\")\n",
    "    print(\"  2. 已安装 modelscope: pip install -U modelscope\")\n",
    "    print(\"  3. 已安装 onnx: pip install -U onnx onnxruntime\")\n",
    "    print(\"  4. 网络连接正常，可以下载模型\")\n",
    "    print(\"  5. PyTorch 版本兼容（建议 2.0+）\")\n",
    "    print(\"=\" * 80)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79749535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens 8404\n"
     ]
    }
   ],
   "source": [
    "# 转换 tokens.json 为 tokens.txt\n",
    "\n",
    "import sys\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def load_tokens():\n",
    "    ans = dict()\n",
    "    i = 0\n",
    "    with open(f\"{model_dir}/tokens.json\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if '[' in line: continue\n",
    "            if ']' in line: continue\n",
    "            if '\"' in line and ',' in line:\n",
    "              line = line[1:-2]\n",
    "\n",
    "            ans[i] = line.strip()\n",
    "            i += 1\n",
    "    print('num tokens', i)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def write_tokens(tokens: Dict[int, str]):\n",
    "    with open(f\"{model_dir}/tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, s in tokens.items():\n",
    "            f.write(f\"{s} {idx}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    tokens = load_tokens()\n",
    "    write_tokens(tokens)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa692be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ./speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.onnx\n"
     ]
    }
   ],
   "source": [
    "# 添加元数据到 ONNX 模型\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "\n",
    "\n",
    "def load_cmvn():\n",
    "    neg_mean = None\n",
    "    inv_stddev = None\n",
    "\n",
    "    with open(f\"{model_dir}/am.mvn\") as f:\n",
    "        for line in f:\n",
    "            if not line.startswith(\"<LearnRateCoef>\"):\n",
    "                continue\n",
    "            t = line.split()[3:-1]\n",
    "\n",
    "            if neg_mean is None:\n",
    "                neg_mean = \",\".join(t)\n",
    "            else:\n",
    "                inv_stddev = \",\".join(t)\n",
    "\n",
    "    return neg_mean, inv_stddev\n",
    "\n",
    "\n",
    "def load_lfr_params():\n",
    "    with open(f\"{model_dir}/config.yaml\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \"lfr_m\" in line:\n",
    "                lfr_m = int(line.split()[-1])\n",
    "            elif \"lfr_n\" in line:\n",
    "                lfr_n = int(line.split()[-1])\n",
    "                break\n",
    "    lfr_window_size = lfr_m\n",
    "    lfr_window_shift = lfr_n\n",
    "    return lfr_window_size, lfr_window_shift\n",
    "\n",
    "\n",
    "def get_vocab_size():\n",
    "    with open(f\"{model_dir}/tokens.txt\", encoding=\"utf-8\") as f:\n",
    "        return len(f.readlines())\n",
    "\n",
    "\n",
    "def add_meta_data(filename: str, meta_data: Dict[str, str]):\n",
    "    \"\"\"Add meta data to an ONNX model. It is changed in-place.\n",
    "    Args:\n",
    "      filename:\n",
    "        Filename of the ONNX model to be changed.\n",
    "      meta_data:\n",
    "        Key-value pairs.\n",
    "    \"\"\"\n",
    "    model = onnx.load(filename)\n",
    "    for key, value in meta_data.items():\n",
    "        meta = model.metadata_props.add()\n",
    "        meta.key = key\n",
    "        meta.value = value\n",
    "\n",
    "    onnx.save(model, filename)\n",
    "    print(f\"Updated {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    lfr_window_size, lfr_window_shift = load_lfr_params()\n",
    "    neg_mean, inv_stddev = load_cmvn()\n",
    "    vocab_size = get_vocab_size()\n",
    "\n",
    "    meta_data = {\n",
    "        \"lfr_window_size\": str(lfr_window_size),\n",
    "        \"lfr_window_shift\": str(lfr_window_shift),\n",
    "        \"neg_mean\": neg_mean,\n",
    "        \"inv_stddev\": inv_stddev,\n",
    "        \"model_type\": \"paraformer\",\n",
    "        \"version\": \"1\",\n",
    "        \"model_author\": \"iic\",\n",
    "        \"vocab_size\": str(vocab_size),\n",
    "        \"description\": \"This is a Chinese model. It supports only Chinese\",\n",
    "        \"comment\": \"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8358-tensorflow1\",\n",
    "        \"git_tag\": \"v1.1.9\",\n",
    "        \"url\": \"https://www.modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8358-tensorflow1\",\n",
    "    }\n",
    "    add_meta_data(f\"{model_dir}/model.onnx\", meta_data)\n",
    "    # add_meta_data(f\"{model_dir}/model.int8.onnx\", meta_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
